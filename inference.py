import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

def main():
    # Modelin path'ini belirt (fine-tuned model burada olmalÄ±)
    model_path = "./mistral-finetuned"

    # Tokenizer ve model yÃ¼kle
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path, 
        torch_dtype=torch.float16, 
        device_map="auto"
    )

    print("ğŸ² Kutu Oyunu TasarÄ±m Modeli Aktif!")
    print("Prompt girin (Ã§Ä±kmak iÃ§in 'q' yazÄ±n):")

    while True:
        user_input = input("\nğŸ“ KullanÄ±cÄ± girdisi: ")
        if user_input.lower() == 'q':
            print("Ã‡Ä±kÄ±lÄ±yor...")
            break

        # Prompt'u oluÅŸtur (istersen sistem mesajÄ± ya da gÃ¶rev ekleyebilirsin)
        prompt = f"KullanÄ±cÄ± girdisi: {user_input}\nGÃ¶rev: KullanÄ±cÄ±nÄ±n ilgi alanlarÄ±na gÃ¶re Ã¶zgÃ¼n bir kutu oyunu Ã¶ner."

        # Modelden yanÄ±t al
        inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
        output = model.generate(
            **inputs,
            max_new_tokens=512,
            do_sample=True,
            temperature=0.8,
            top_p=0.95,
            pad_token_id=tokenizer.eos_token_id
        )

        decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)
        print("\nğŸ® Ã–nerilen Oyun TasarÄ±mÄ±:\n")
        print(decoded_output.split("GÃ¶rev:")[-1].strip())

if __name__ == "__main__":
    main()
